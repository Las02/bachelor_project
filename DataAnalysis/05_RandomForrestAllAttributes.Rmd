---
title: ""
output:
  pdf_document: default
  html_document: default
date: "2023-04-06"
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(magrittr)
library(tidymodels)
library(cowplot)
library(modelr)
library(naniar)
```

## Reading in data

```{r}
D <- readRDS("./data/bacdive_growth_ribdif.rds")
# for the Pathogens, set nan to 0
Dt_tmp1 <- D %>% 
  mutate(pathogen_any = ifelse(is.na(pathogen_any), "P0", pathogen_any))

# Set nan to neutrophile for PH.range
Dt_tmp2 <- Dt_tmp1 %>% 
  mutate(PH.range = ifelse(is.na(PH.range), "Pneutrophile", PH.range))

# convert pseudogenes to percent
Dt_tmp3 <- Dt_tmp2 %>% 
  mutate(pseudogenes_percent = 100*(pseudogenes/total_genes))

# Only select data for Bacilliota or Actinomycetota
D_all <- Dt_tmp3 %>%  
  filter(phylum %in% c("Bacillota","Actinomycetota")) %>% 
  select(species,last_col(42):last_col())  %>% 
  mutate(growth_temp = as.double(growth_temp)) 

# Filter out nans for sporeforming
D_filtered_tmp <- D_all %>% 
  filter(!is.na(sporeforming))


```

# Select the correct attributes

```{r}
D_filtered <- D_filtered_tmp %>% 
  select(oxygen.tolerance, growth_temp, n16, 
         gc_percent, sporeforming, total_seq_length, genome_components, pseudogenes_percent, ar_count) %>% 
  filter(if_all(everything(), ~!is.na(.x))) 

```


### Spliting data

Using strata =, to make sure that sporeforming is equally distributed

```{r}
set.seed(07042023)
D_split <- initial_split(D_filtered, prop = 0.80, strata = sporeforming)
D_train <- training(D_split) 
D_test <- testing(D_split)
```

### Setting up model

Based on: <https://juliasilge.com/blog/sf-trees-random-tuning/> 

```{r}
# setting up recipe
tree_rec <- recipe(sporeforming ~., data=D_train)

# Setting up model hyperparameters
tune_spec <- rand_forest(
  # number which can be sampled per tree
  mtry = tune(),
  trees = 1000,
  # min number of datapoints for a split being made 
  min_n = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("ranger")


# setting up workflow
tune_wf <- workflow() %>% 
  add_recipe(tree_rec) %>% 
  add_model(tune_spec)

```

## Tune hyperparameters

```{r}
# Set to parallel
doParallel::registerDoParallel()

# Make samples for cross-validation
trees_folds <- vfold_cv(D_train)

# Tune hyperparameters
# grid sets the number of data points to try
tune_res <- tune_grid(
  tune_wf,
  # Sets the 
  resamples = trees_folds,
  grid = 20
)

```

## Picking the best model :)

### First run

```{r}
tune_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")
```

Lets try to run model on more values in the interval which have many ok scores.

### Second run

```{r}
# Making grid again based on above
rf_grid <- grid_regular(
  min_n(range = c(1,6)),
  mtry(range=c(1, 5)),
  levels = 5
)

# And run the model on the grid
regular_res <- tune_grid(
  tune_wf,
  resamples = trees_folds,
  grid = rf_grid
)

```

Results

```{r}
regular_res %>% 
  collect_metrics()

```

AUC

```{r}
regular_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "AUC")
```


finalize the model

```{r}
best_auc <- select_best(regular_res, "roc_auc")

final_rf <- finalize_model(
  tune_spec,
  best_auc
)

final_rf
# mtry = 2
# min_n = 1
```

# Varible importance plot
```{r}
library(vip)

final_rf %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(factor(sporeforming) ~ . ,data=D_train) %>%
  vip(geom = "point")
```

Now lets fit final data on entire training set and evaluate on testing
data

```{r}
final_wf <- workflow() %>%
  add_recipe(tree_rec) %>%
  add_model(final_rf)

final_res <- final_wf %>%
  last_fit(D_split)

final_res %>%
  collect_metrics()

# Accuracy 0.93
# Roc_auc 0.98

```









